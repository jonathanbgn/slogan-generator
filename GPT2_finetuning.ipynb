{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l9tTd8E8WXEm"
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "MODEL_NAME = 'distilgpt2' #'distilgpt2' 'gpt2-medium'\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(MODEL_NAME)\n",
    "model = GPT2LMHeadModel.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "JjAPK7wpYjg4",
    "outputId": "21a2a352-26ac-4a8c-896a-424baa9eb8a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<pad>', 'additional_special_tokens': ['<context>', '<slogan>']}\n"
     ]
    }
   ],
   "source": [
    "# Declare special tokens for padding and separating the context from the slogan:\n",
    "SPECIAL_TOKENS_DICT = {\n",
    "    'pad_token': '<pad>',\n",
    "    'additional_special_tokens': ['<context>', '<slogan>'],\n",
    "}\n",
    "\n",
    "# Add these special tokens to the vocabulary and resize model's embeddings:\n",
    "tokenizer.add_special_tokens(SPECIAL_TOKENS_DICT)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Show the full list of special tokens:\n",
    "print(tokenizer.special_tokens_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "FKBEnqJsiN-Z",
    "outputId": "5689891f-9ddd-4e33-ca9a-3eb852eff1d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 64])\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class SloganDataset(Dataset):\n",
    "  def __init__(self, filename, tokenizer, seq_length=64):\n",
    "\n",
    "    context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "    slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "    pad_tkn = tokenizer.pad_token_id\n",
    "    eos_tkn = tokenizer.eos_token_id\n",
    "\n",
    "    self.examples = []\n",
    "    with open(filename) as csvfile:\n",
    "      reader = csv.reader(csvfile)\n",
    "      for row in reader:\n",
    "      \n",
    "        # Build the context and slogan segments:\n",
    "        context = [context_tkn] + tokenizer.encode(row[0], max_length=seq_length//2-1)\n",
    "        slogan = [slogan_tkn] + tokenizer.encode(row[1], max_length=seq_length//2-2) + [eos_tkn]\n",
    "        \n",
    "        # Concatenate the two parts together:\n",
    "        tokens = context + slogan + [pad_tkn] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Annotate each token with its corresponding segment:\n",
    "        segments = [context_tkn] * len(context) + [slogan_tkn] * ( seq_length - len(context) )\n",
    "\n",
    "        # Ignore the context, padding, and <slogan> tokens by setting their labels to -1\n",
    "        labels = [-1] * (len(context)+1) + slogan[1:] + [-1] * ( seq_length - len(context) - len(slogan) )\n",
    "\n",
    "        # Add the preprocessed example to the dataset\n",
    "        self.examples.append((tokens, segments, labels))\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.examples)\n",
    "\n",
    "  def __getitem__(self, item):\n",
    "    return torch.tensor(self.examples[item])\n",
    "\n",
    "\n",
    "# Build the dataset and display the dimensions of the 1st batch for verification:\n",
    "slogan_dataset = SloganDataset('slogans.csv', tokenizer)\n",
    "print(next(iter(slogan_dataset)).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7XSTxJHYKLE"
   },
   "outputs": [],
   "source": [
    "import math, random\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "\n",
    "# Create data indices for training and validation splits:\n",
    "\n",
    "indices = list(range(len(slogan_dataset)))\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(indices)\n",
    "\n",
    "split = math.floor(0.1 * len(slogan_dataset))\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "# Build the PyTorch data loaders:\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = DataLoader(slogan_dataset, batch_size=32, sampler=train_sampler)\n",
    "val_loader = DataLoader(slogan_dataset, batch_size=64, sampler=val_sampler)\n",
    "# Note: we can double the batch size for validation since no backprogation is involved (thus it will fit on GPU's memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9t2RwWNgE4l"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def fit(model, optimizer, train_dl, val_dl, epochs=1, device=torch.device('cpu')):\n",
    "\n",
    "  for i in range(epochs):\n",
    "\n",
    "    print('\\n--- Starting epoch #{} ---'.format(i))\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # These 2 lists will keep track of the batch losses and batch sizes over one epoch:\n",
    "    losses = []\n",
    "    nums = []\n",
    "\n",
    "    for xb in tqdm(train_dl, desc=\"Training\"):\n",
    "      # Move the batch to the training device:\n",
    "      inputs = xb.to(device)\n",
    "\n",
    "      # Call the model with the token ids, segment ids, and the ground truth (labels)\n",
    "      outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "      \n",
    "      # Add the loss and batch size to the list:\n",
    "      loss = outputs[0]\n",
    "      losses.append(loss.item())\n",
    "      nums.append(len(xb))\n",
    "\n",
    "      loss.backward()\n",
    "\n",
    "      optimizer.step()\n",
    "      model.zero_grad()\n",
    "\n",
    "    # Compute the average cost over one epoch:\n",
    "    train_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "\n",
    "    # Now do the same thing for validation:\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "      losses = []\n",
    "      nums = []\n",
    "\n",
    "      for xb in tqdm(val_dl, desc=\"Validation\"):\n",
    "        inputs = xb.to(device)\n",
    "        outputs = model(inputs[:,0,:], token_type_ids=inputs[:,1,:], labels=inputs[:,2,:])\n",
    "        losses.append(outputs[0].item())\n",
    "        nums.append(len(xb))\n",
    "\n",
    "    val_cost = np.sum(np.multiply(losses, nums)) / sum(nums)\n",
    "\n",
    "    print('\\n--- Epoch #{} finished --- Training cost: {} / Validation cost: {}'.format(i, train_cost, val_cost))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "colab_type": "code",
    "id": "bPvm_dkUz8I0",
    "outputId": "68dce6da-dd15-45b7-920c-a5f73298517f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting epoch #0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:50<00:00,  5.71it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:01<00:00,  8.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #0 finished --- Training cost: 4.139444000803344 / Validation cost: 3.2550279853724633\n",
      "\n",
      "--- Starting epoch #1 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 268/268 [00:50<00:00,  5.71it/s]\n",
      "Validation: 100%|██████████| 15/15 [00:01<00:00,  8.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Epoch #1 finished --- Training cost: 2.6903427139161127 / Validation cost: 3.2779046367196476\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "# Move the model to the GPU:\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "\n",
    "# Fine-tune GPT2 for two epochs:\n",
    "optimizer = AdamW(model.parameters())\n",
    "fit(model, optimizer, train_loader, val_loader, epochs=2, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-cS_1D0tZDGG"
   },
   "outputs": [],
   "source": [
    "# Sampling functions with top k and top p from HuggingFace:\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "def top_k_top_p_filtering(logits, top_k=0, top_p=0.0, filter_value=-float('Inf')):\n",
    "    \"\"\" Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
    "        Args:\n",
    "            logits: logits distribution shape (batch size x vocabulary size)\n",
    "            top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
    "            top_p > 0.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
    "                Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
    "        From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
    "    \"\"\"\n",
    "    top_k = min(top_k, logits.size(-1))  # Safety check\n",
    "    if top_k > 0:\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
    "        logits[indices_to_remove] = filter_value\n",
    "\n",
    "    if top_p > 0.0:\n",
    "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "        # Remove tokens with cumulative probability above the threshold\n",
    "        sorted_indices_to_remove = cumulative_probs > top_p\n",
    "        # Shift the indices to the right to keep also the first token above the threshold\n",
    "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "        sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "        # scatter sorted tensors to original indexing\n",
    "        indices_to_remove = sorted_indices_to_remove.scatter(dim=1, index=sorted_indices, src=sorted_indices_to_remove)\n",
    "        logits[indices_to_remove] = filter_value\n",
    "    return logits\n",
    "\n",
    "\n",
    "# From HuggingFace, adapted to work with the context/slogan separation:\n",
    "def sample_sequence(model, length, context, segments_tokens=None, num_samples=1, temperature=1, top_k=0, top_p=0.0, repetition_penalty=1.0,\n",
    "                    device='cpu'):\n",
    "    context = torch.tensor(context, dtype=torch.long, device=device)\n",
    "    context = context.unsqueeze(0).repeat(num_samples, 1)\n",
    "    generated = context\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in trange(length):\n",
    "\n",
    "            inputs = {'input_ids': generated}\n",
    "            if segments_tokens != None:\n",
    "              inputs['token_type_ids'] = torch.tensor(segments_tokens[:generated.shape[1]]).unsqueeze(0).repeat(num_samples, 1)\n",
    "\n",
    "\n",
    "            outputs = model(**inputs)  # Note: we could also use 'past' with GPT-2/Transfo-XL/XLNet/CTRL (cached hidden-states)\n",
    "            next_token_logits = outputs[0][:, -1, :] / (temperature if temperature > 0 else 1.)\n",
    "\n",
    "            # repetition penalty from CTRL (https://arxiv.org/abs/1909.05858)\n",
    "            for i in range(num_samples):\n",
    "                for _ in set(generated[i].tolist()):\n",
    "                    next_token_logits[i, _] /= repetition_penalty\n",
    "                \n",
    "            filtered_logits = top_k_top_p_filtering(next_token_logits, top_k=top_k, top_p=top_p)\n",
    "            if temperature == 0: # greedy sampling:\n",
    "                next_token = torch.argmax(filtered_logits, dim=-1).unsqueeze(-1)\n",
    "            else:\n",
    "                next_token = torch.multinomial(F.softmax(filtered_logits, dim=-1), num_samples=1)\n",
    "            generated = torch.cat((generated, next_token), dim=1)\n",
    "    return generated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "colab_type": "code",
    "id": "A1EjyJgCaqYN",
    "outputId": "90854ab2-a7dc-41c9-a5e4-fc9cefecf536"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:11<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- Generated Slogans ---\n",
      "\n",
      " Are you a big deal? Recipes choose Starbucks.\n",
      " Fit for yourself. Have a cup of Starbucks. Have a cup of coffee.\n",
      " Kick start, start. Loaded fresh.\n",
      " Sheer coffee with touch.\n",
      " Starbucks. Everywhere you are.\n",
      " Starbucks knows its best when it's Starbucks.\n",
      " Great coffee from our staff.\n",
      " Starbucks bean clarity since 1996.\n",
      " Serious coffee should never be given the same.\n",
      " America's Starbucks moment.\n",
      " Starbucks. Digest. Locale.\n",
      " Superior coffee taste, life, and more.\n",
      " Great coffee comes from country out.\n",
      " Fine coffee, great coffee, lovely service.\n",
      " Intaste the bean. Real coffee.\n",
      " Imagine the difference in Starbucks.\n",
      " Pick it up!\n",
      " Time is not enough.\n",
      " Experience. Enjoy. Pleasure.\n",
      " Inspiring science.\n"
     ]
    }
   ],
   "source": [
    "context = \"Starbucks, coffee chain from Seattle\"\n",
    "\n",
    "context_tkn = tokenizer.additional_special_tokens_ids[0]\n",
    "slogan_tkn = tokenizer.additional_special_tokens_ids[1]\n",
    "\n",
    "input_ids = [context_tkn] + tokenizer.encode(context)\n",
    "\n",
    "segments = [slogan_tkn] * 64\n",
    "segments[:len(input_ids)] = [context_tkn] * len(input_ids)\n",
    "\n",
    "input_ids += [slogan_tkn]\n",
    "\n",
    "# Move the model back to the CPU for inference:\n",
    "model.to(torch.device('cpu'))\n",
    "\n",
    "# Generate 20 samples of max length 20\n",
    "generated = sample_sequence(model, length=20, context=input_ids, segments_tokens=segments, num_samples=20)\n",
    "\n",
    "print('\\n\\n--- Generated Slogans ---\\n')\n",
    "\n",
    "for g in generated:\n",
    "  slogan = tokenizer.decode(g.squeeze().tolist())\n",
    "  slogan = slogan.split('<|endoftext|>')[0].split('<slogan>')[1]\n",
    "  print(slogan)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Slogan Generator GPT2 HuggingFace.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
